{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 12 21:05:53 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2070 ...    Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   84C    P0             65W /   80W |     971MiB /   8192MiB |     19%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12\n",
    "#%pip install cucim-cu12 cupy-cuda12x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:05:53.685249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-12 21:05:53.706561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-12 21:05:53.712705: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 21:05:53.729125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is torch using cuda?  True\n",
      "Is tensorflow using cuda?  True\n",
      "Is pandas using cuda?  <module 'pandas' from '/usr/local/lib/python3.11/dist-packages/pandas/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "#%load_ext cudf.pandas\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "#from cucim.skimage.exposure import rescale_intensity\n",
    "import tensorflow as tf\n",
    "#import cupy as cp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Is torch using cuda? \",torch.cuda.is_available())\n",
    "print(\"Is tensorflow using cuda? \",tf.test.is_built_with_cuda())\n",
    "print(\"Is pandas using cuda? \",pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mapping = [\n",
    "    \"box\",\n",
    "    \"circularTorus\",\n",
    "    \"cone\",\n",
    "    \"coneOffset\",\n",
    "    \"cylinder\",\n",
    "    \"cylinderSlope\",\n",
    "    \"dish\",\n",
    "    \"mesh\",\n",
    "    \"pyramid\",\n",
    "    \"rectangularTorus\",\n",
    "    \"sphere\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_number(texts:list[str]):\n",
    "    def key(text:str):\n",
    "        text = re.sub(r'.*photos_', '', text)\n",
    "        text = re.sub(r'\\.csv', '', text)\n",
    "        text = re.sub(r'\\D', '', text)\n",
    "        return int(text)\n",
    "    return sorted(texts, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "base_path = '/home/workspace/geometry-classifier/data/'\n",
    "parquet_files = sorted(glob.glob(base_path + 'photos_v3_parquet/*.parquet'))\n",
    "\n",
    "def data_generator():\n",
    "    counter = 0\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    for file in parquet_files:\n",
    "        df = pd.read_parquet(file)\n",
    "        \n",
    "        # Drop unnecessary columns and encode labels\n",
    "        df = df.drop(columns=['id'], axis=1)\n",
    "        df['name'] = encoder.fit_transform(df['name'])\n",
    "        \n",
    "        # Split data into train, validation, and test sets\n",
    "        X_train, X_aux, y_train, y_aux = train_test_split(df.drop(columns=[\"name\"]), df['name'], test_size=0.4, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Ensure consistency between X and Y sizes\n",
    "        if len(X_train) != len(y_train) or len(X_val) != len(y_val):\n",
    "            raise ValueError(\"Mismatch in number of samples between features and labels\")\n",
    "        \n",
    "        # Save training data every 5 iterations\n",
    "        if counter % 5 == 0:\n",
    "            training_file = f\"training_{counter // 5}.parquet\"\n",
    "            training_df = pd.DataFrame(X_train)\n",
    "            training_df['name'] = y_train\n",
    "            training_df.to_parquet(base_path + f\"training/{training_file}\")\n",
    "        \n",
    "        # Efficiently reshape each row to (224, 224, 1)\n",
    "        X = np.array([x.reshape(224, 224, 1).astype(np.uint8) for x in X_train.to_numpy()])\n",
    "        X_val = np.array([x.reshape(224, 224, 1).astype(np.uint8) for x in X_val.to_numpy()])\n",
    "        \n",
    "        # Ensure labels are numpy arrays\n",
    "        Y = y_train.values\n",
    "        y_val = y_val.values\n",
    "\n",
    "        yield X, Y, X_val, y_val\n",
    "        \n",
    "        # Cleanup to free memory\n",
    "        del df, X_train, y_train, X_test, y_test, Y, X, X_val, y_val, X_aux, y_aux\n",
    "        gc.collect()\n",
    "        \n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731445558.760387     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731445558.768062     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731445558.768297     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731445558.769489     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731445558.769704     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731445558.769878     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731445558.870699     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731445558.871138     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1731445558.871501     445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-12 21:05:58.871878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5831 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# 224 x 224\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = models.Sequential([\n",
    "    # First Convolutional Block with reduced filters\n",
    "    layers.Conv2D(8, (3, 3), activation='relu', input_shape=(224, 224, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second Convolutional Block with reduced filters\n",
    "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third Convolutional Block with reduced filters\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Global Average Pooling instead of Flatten\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    \n",
    "    # Final Dense Layers\n",
    "    layers.Dense(32, activation='relu'),  # Reduced dense layer size\n",
    "    layers.Dropout(0.4),  # Dropout for regularization\n",
    "    layers.Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731445830.015846     512 service.cc:146] XLA service 0x770798005900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731445830.015888     512 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 with Max-Q Design, Compute Capability 7.5\n",
      "2024-11-12 21:10:30.075586: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-12 21:10:30.355581: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  35/1042\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.2873"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731445833.685589     512 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  77/1042\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.2685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:10:36.727119: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-11-12 21:10:36.727218: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "/usr/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.16544, saving model to model.keras\n",
      "\u001b[1m1042/1042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.2373 - val_accuracy: 1.0000 - val_loss: 2.1654\n",
      "\u001b[1m  70/1042\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.1352\n",
      "Epoch 1: val_loss improved from 2.16544 to 2.03179, saving model to model.keras\n",
      "\u001b[1m1042/1042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558us/step - accuracy: 1.0000 - loss: 2.1018 - val_accuracy: 1.0000 - val_loss: 2.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:11:09.925683: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-11-12 21:11:09.925741: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11082665919636872685\n",
      "/usr/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-11-12 21:11:09.925764: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13971001384966430482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  70/1042\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.0025\n",
      "Epoch 1: val_loss improved from 2.03179 to 1.90209, saving model to model.keras\n",
      "\u001b[1m1042/1042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 464us/step - accuracy: 1.0000 - loss: 1.9700 - val_accuracy: 1.0000 - val_loss: 1.9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:11:37.434991: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11082665919636872685\n",
      "2024-11-12 21:11:37.435038: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13971001384966430482\n",
      "/usr/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  66/1042\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.8753\n",
      "Epoch 1: val_loss improved from 1.90209 to 1.77672, saving model to model.keras\n",
      "\u001b[1m1042/1042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507us/step - accuracy: 1.0000 - loss: 1.8423 - val_accuracy: 1.0000 - val_loss: 1.7767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 21:12:06.017105: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2024-11-12 21:12:06.017151: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11082665919636872685\n",
      "/usr/lib/python3.11/contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-11-12 21:12:06.017174: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13971001384966430482\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model copy.ipynb Célula 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m X, Y, X_val, Y_val \u001b[39min\u001b[39;49;00m data_generator():\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         cnn \u001b[39m=\u001b[39;49m model\u001b[39m.\u001b[39;49mfit(X,Y, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(parquet_files), callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback], batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_val, Y_val), verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model copy.ipynb Célula 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m encoder \u001b[39m=\u001b[39m LabelEncoder()\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m parquet_files:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Drop unnecessary columns and encode labels\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/hd/Repos/Github/VIS-3D-ZIP/geometry-classifier/luna_model%20copy.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    668\u001b[0m     path,\n\u001b[1;32m    669\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m    670\u001b[0m     filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m    671\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    672\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    673\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    674\u001b[0m     filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m    675\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    676\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py:403\u001b[0m, in \u001b[0;36mFastParquetImpl.read\u001b[0;34m(self, path, columns, filters, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     parquet_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mParquetFile(path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparquet_kwargs)\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m parquet_file\u001b[39m.\u001b[39;49mto_pandas(columns\u001b[39m=\u001b[39;49mcolumns, filters\u001b[39m=\u001b[39;49mfilters, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    404\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/fastparquet/api.py:790\u001b[0m, in \u001b[0;36mParquetFile.to_pandas\u001b[0;34m(self, columns, categories, filters, index, row_filter, dtypes)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     parts \u001b[39m=\u001b[39m {name: (v \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m-catdef\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    788\u001b[0m                     \u001b[39melse\u001b[39;00m v[start:start \u001b[39m+\u001b[39m thislen])\n\u001b[1;32m    789\u001b[0m              \u001b[39mfor\u001b[39;00m (name, v) \u001b[39min\u001b[39;00m views\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 790\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_row_group_file(rg, columns, categories, index,\n\u001b[1;32m    791\u001b[0m                              assign\u001b[39m=\u001b[39;49mparts, partition_meta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartition_meta,\n\u001b[1;32m    792\u001b[0m                              row_filter\u001b[39m=\u001b[39;49msel, infile\u001b[39m=\u001b[39;49minfile)\n\u001b[1;32m    793\u001b[0m     start \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m thislen\n\u001b[1;32m    794\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/fastparquet/api.py:388\u001b[0m, in \u001b[0;36mParquetFile.read_row_group_file\u001b[0;34m(self, rg, columns, categories, index, assign, partition_meta, row_filter, infile)\u001b[0m\n\u001b[1;32m    385\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    386\u001b[0m f \u001b[39m=\u001b[39m infile \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(fn, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 388\u001b[0m core\u001b[39m.\u001b[39;49mread_row_group(\n\u001b[1;32m    389\u001b[0m     f, rg, columns, categories, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mschema, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcats,\n\u001b[1;32m    390\u001b[0m     selfmade\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselfmade, index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    391\u001b[0m     assign\u001b[39m=\u001b[39;49massign, scheme\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_scheme, partition_meta\u001b[39m=\u001b[39;49mpartition_meta,\n\u001b[1;32m    392\u001b[0m     row_filter\u001b[39m=\u001b[39;49mrow_filter\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m ret:\n\u001b[1;32m    395\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/fastparquet/core.py:645\u001b[0m, in \u001b[0;36mread_row_group\u001b[0;34m(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme, partition_meta, row_filter)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39mif\u001b[39;00m assign \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mGoing with pre-allocation!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 645\u001b[0m read_row_group_arrays(file, rg, columns, categories, schema_helper,\n\u001b[1;32m    646\u001b[0m                       cats, selfmade, assign\u001b[39m=\u001b[39;49massign, row_filter\u001b[39m=\u001b[39;49mrow_filter)\n\u001b[1;32m    648\u001b[0m \u001b[39mfor\u001b[39;00m cat \u001b[39min\u001b[39;00m cats:\n\u001b[1;32m    649\u001b[0m     \u001b[39mif\u001b[39;00m cat \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m assign:\n\u001b[1;32m    650\u001b[0m         \u001b[39m# do no need to have partition columns in output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/fastparquet/core.py:613\u001b[0m, in \u001b[0;36mread_row_group_arrays\u001b[0;34m(file, rg, columns, categories, schema_helper, cats, selfmade, assign, row_filter)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m columns \u001b[39mor\u001b[39;00m name \u001b[39min\u001b[39;00m cats:\n\u001b[1;32m    612\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m remains\u001b[39m.\u001b[39mdiscard(name)\n\u001b[1;32m    615\u001b[0m read_col(column, schema_helper, file, use_cat\u001b[39m=\u001b[39mname\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-catdef\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m out,\n\u001b[1;32m    616\u001b[0m          selfmade\u001b[39m=\u001b[39mselfmade, assign\u001b[39m=\u001b[39mout[name],\n\u001b[1;32m    617\u001b[0m          catdef\u001b[39m=\u001b[39mout\u001b[39m.\u001b[39mget(name\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-catdef\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    618\u001b[0m          row_filter\u001b[39m=\u001b[39mrow_filter)\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m _is_map_like(schema_helper, column):\n\u001b[1;32m    621\u001b[0m     \u001b[39m# TODO: could be done in fast loop in _assemble_objects?\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint('model.keras',save_best_only=True, save_weights_only=False, mode='min', verbose=1)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"Epochs {i+1}/{epochs}\")\n",
    "    for X_train, Y_train, X_val, Y_val in data_generator():\n",
    "        cnn = model.fit(X_train,Y_train, epochs=1, steps_per_epoch=len(parquet_files), callbacks=[checkpoint_callback], batch_size=8, validation_data=(X_val, Y_val), verbose=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
